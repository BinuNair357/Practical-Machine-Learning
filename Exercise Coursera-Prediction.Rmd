---
title: "Prediction Project-Exercise data"
author: "Binu Nair"
date: "13 April 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(lattice)
library(ggplot2)
library(plyr)
library(randomForest)
library(caret)
library(rpart)
library(rpart.plot)
```

## Executive Summary

Based on a dataset provide by HAR [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har) we will try to train a predictive model to predict what exercise was performed using the dataset

We'll take the following steps:

- Explore the data
- Process the data
- Model selection
- Model examination
- Predicting
- Conclusion
 


```{r}
training_data <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
test_data <- read.csv("pml-testing.csv", na.strings = c("NA", "#DIV/0!", ""))
```

## Exploratory data analyses 

Look at the dimensions & head of the dataset to get an idea
```{r}
# Res 1
dim(training_data)

head(training_data)

str(training_data)

summary(training_data)
```


What we see is a lot of data with NA / empty values.We first remove those data contains more than 95% of the observation to be NA.
Let's remove those

```{r}
clnColumnIndex <- colSums(is.na(training_data))/nrow(training_data) < 0.95
clean_training_data <- training_data[,clnColumnIndex]
colSums(is.na(clean_training_data))/nrow(clean_training_data)
colSums(is.na(clean_training_data))
```

We also remove col1 to col7 because they are not related to the model

```{r}
clean_training_data <- clean_training_data[,-c(1:7)]
clean_test_data <- test_data[,-c(1:7)]
```

We then partition the training data into training set and cross validation set
```{r}
inTrainIndex <- createDataPartition(clean_training_data$classe, p=0.75)[[1]]
training_training_data <- clean_training_data[inTrainIndex,]
training_crossval_data <- clean_training_data[-inTrainIndex,]
```

change the test data set into the same

```{r}
allNames <- names(clean_training_data)
clean_test_data <- test_data[,allNames[1:52]]
```

# Decision Tree
```{r}
decisionTreeMod <- train(classe ~., method='rpart', data=training_training_data)
decisionTreePrediction <- predict(decisionTreeMod, training_crossval_data)
confusionMatrix(training_crossval_data$classe, decisionTreePrediction)
```
plotting the decision tree
```{r}
rpart.plot(decisionTreeMod$finalModel)
```
# Random Forest
```{r}
rfMod <- train(classe ~., method='rf', data=training_training_data, ntree=128)
rfPrediction <- predict(rfMod, training_crossval_data)
confusionMatrix(training_crossval_data$classe, rfPrediction)
```
# Prediction
```{r}
predict(rfMod, clean_test_data)
```
# Conclusion
The random forest algorithem far outperforms the decision tree in terms of accuracy. We are getting 99.25% in sample accuracy, while the decision tree gives us only nearly 50% in sample accuracy
